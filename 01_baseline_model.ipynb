{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d53f00",
   "metadata": {},
   "source": [
    "# Return Prediction Baseline Model\n",
    "Using our real ASOS Graphics + TheLook e-commerce data from our group project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751cfae-a5d8-4f79-975a-9e7a1ba73531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order items: (180952, 11)\n",
      "Products:    (29120, 9)\n",
      "Users:       (100000, 15)\n",
      "Return rate (from status): 25.0%\n",
      "Merged df: (180952, 37)\n",
      "Feature matrix after encoding: (180952, 2789)\n",
      "Train shape: (144761, 2789), Test shape: (36191, 2789)\n",
      "Subsampled training data to 80000 rows\n",
      "scale_pos_weight: 3.00\n",
      "\n",
      "=== MODEL RESULTS ===\n",
      "F1-score  : 0.334\n",
      "ROC-AUC   : 0.536\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71     27127\n",
      "           1       0.28      0.41      0.33      9064\n",
      "\n",
      "    accuracy                           0.59     36191\n",
      "   macro avg       0.53      0.53      0.52     36191\n",
      "weighted avg       0.65      0.59      0.61     36191\n",
      "\n",
      "\n",
      "Model saved → src/model/return_predictor.json\n",
      "Feature names saved → src/model/feature_names.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===============================\n",
    "# 1. LOAD YOUR AVAILABLE DATASETS\n",
    "# ===============================\n",
    "\n",
    "order_items  = pd.read_csv(\"data/order_items.csv\")\n",
    "products     = pd.read_csv(\"data/products.csv\")\n",
    "users        = pd.read_csv(\"data/users.csv\")\n",
    "\n",
    "print(f\"Order items: {order_items.shape}\")\n",
    "print(f\"Products:    {products.shape}\")\n",
    "print(f\"Users:       {users.shape}\")\n",
    "\n",
    "# ===============================\n",
    "# 2. CREATE TARGET VARIABLE\n",
    "# ===============================\n",
    "# Using status from order_items\n",
    "order_items[\"is_returned\"] = order_items[\"status\"].isin([\"Returned\", \"Cancelled\"]).astype(int)\n",
    "print(f\"Return rate (from status): {order_items['is_returned'].mean():.1%}\")\n",
    "\n",
    "# ===============================\n",
    "# 3. FIX DATETIME PARSING (UTC + MIXED FORMAT)\n",
    "# ===============================\n",
    "\n",
    "order_items[\"created_at\"] = pd.to_datetime(\n",
    "    order_items[\"created_at\"],\n",
    "    format=\"mixed\",\n",
    "    utc=True,\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "order_items[\"delivered_at\"] = pd.to_datetime(\n",
    "    order_items[\"delivered_at\"],\n",
    "    format=\"mixed\",\n",
    "    utc=True,\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "order_items[\"delivery_time_days\"] = (\n",
    "    order_items[\"delivered_at\"] - order_items[\"created_at\"]\n",
    ").dt.days\n",
    "\n",
    "order_items[\"delivery_time_days\"] = order_items[\"delivery_time_days\"].fillna(30)\n",
    "\n",
    "# ===============================\n",
    "# 4. MERGE PRODUCT + USER FEATURES\n",
    "# ===============================\n",
    "\n",
    "df = order_items.merge(\n",
    "    products,\n",
    "    left_on=\"product_id\",\n",
    "    right_on=\"id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_product\")\n",
    ")\n",
    "\n",
    "df = df.merge(\n",
    "    users,\n",
    "    left_on=\"user_id\",\n",
    "    right_on=\"id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_user\")\n",
    ")\n",
    "\n",
    "print(f\"Merged df: {df.shape}\")\n",
    "\n",
    "# ===============================\n",
    "# 5. SELECT FEATURES\n",
    "# ===============================\n",
    "\n",
    "features = [\n",
    "    \"sale_price\",         # order_items\n",
    "    \"cost\",               # products\n",
    "    \"retail_price\",       # products\n",
    "    \"delivery_time_days\",\n",
    "    \"category\",           # products\n",
    "    \"department\",         # products\n",
    "    \"brand\",              # products\n",
    "    \"age\",                # users\n",
    "    \"traffic_source\",     # users\n",
    "]\n",
    "\n",
    "missing = [col for col in features if col not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df[\"is_returned\"].copy()\n",
    "\n",
    "# We can drop df now to free some RAM\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "# ===============================\n",
    "# 6. ENCODE CATEGORICALS + CLEAN DATA\n",
    "# ===============================\n",
    "\n",
    "cat_cols = [\"category\", \"department\", \"brand\", \"traffic_source\"]\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# Save feature names BEFORE converting to numpy\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "# Memory-friendly numeric NaN filling\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "X[numeric_cols] = X[numeric_cols].fillna(0)\n",
    "\n",
    "print(f\"Feature matrix after encoding: {X.shape}\")\n",
    "\n",
    "# ===============================\n",
    "# 7. CONVERT TO NUMPY (float32) TO REDUCE MEMORY\n",
    "# ===============================\n",
    "\n",
    "X_np = X.to_numpy(dtype=np.float32)\n",
    "y_np = y.to_numpy(dtype=np.int32)\n",
    "\n",
    "# Drop pandas objects to free memory\n",
    "del X\n",
    "del y\n",
    "gc.collect()\n",
    "\n",
    "# ===============================\n",
    "# 8. TRAIN/TEST SPLIT\n",
    "# ===============================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_np, y_np, test_size=0.2, random_state=42, stratify=y_np\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# OPTIONAL: Subsample training data if RAM is still tight\n",
    "max_train_rows = 80000  # you can reduce this to e.g. 50000 if needed\n",
    "if X_train.shape[0] > max_train_rows:\n",
    "    rng = np.random.RandomState(42)\n",
    "    idx = rng.choice(X_train.shape[0], size=max_train_rows, replace=False)\n",
    "    X_train = X_train[idx]\n",
    "    y_train = y_train[idx]\n",
    "    print(f\"Subsampled training data to {X_train.shape[0]} rows\")\n",
    "\n",
    "# ===============================\n",
    "# 9. TRAIN XGBOOST MODEL (LIGHTER CONFIG)\n",
    "# ===============================\n",
    "\n",
    "scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "print(f\"scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=150,          # fewer trees to save memory\n",
    "    max_depth=5,              # shallower trees\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric=\"logloss\",\n",
    "    tree_method=\"hist\",       # memory-efficient histogram-based\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n=== MODEL RESULTS ===\")\n",
    "print(f\"F1-score  : {f1_score(y_test, preds):.3f}\")\n",
    "print(f\"ROC-AUC   : {roc_auc_score(y_test, probs):.3f}\")\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# ===============================\n",
    "# 10. SAVE MODEL + FEATURE NAMES FOR FASTAPI\n",
    "# ===============================\n",
    "\n",
    "os.makedirs(\"src/model\", exist_ok=True)\n",
    "\n",
    "model_path = \"src/model/return_predictor.json\"\n",
    "feature_path = \"src/model/feature_names.pkl\"\n",
    "\n",
    "model.save_model(model_path)\n",
    "joblib.dump(feature_names, feature_path)\n",
    "\n",
    "print(f\"\\nModel saved → {model_path}\")\n",
    "print(f\"Feature names saved → {feature_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc20427-3926-479b-8f7f-c3e2a97b33db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
